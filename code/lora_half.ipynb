{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set os at top \n",
    "import os\n",
    "import torch\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['HF_HOME'] = '/data1/wln/hf_cache'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# load model and processor\n",
    "processor = AutoProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
    "base_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    'Salesforce/blip2-opt-2.7b', \n",
    "    local_files_only=True,\n",
    "    \n",
    ")\n",
    "\n",
    "# set training args \n",
    "training_args = TrainingArguments(\n",
    "    output_dir= 'lora_adapter_loss_x_0312xx',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "\n",
    "# lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16, #8\n",
    "    lora_alpha=32, #16 \n",
    "    lora_dropout=0.05, #0.05\n",
    "    # target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# get model for training\n",
    "adapter_model = get_peft_model(base_model, lora_config)\n",
    "adapter_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataset[index]\n",
    "        encoding = self.processor(\n",
    "            images = item['image'],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        for k,v in encoding.items():  encoding[k] = v.squeeze()\n",
    "        encoding['caption'] = item['caption'][0] \n",
    "\n",
    "        return encoding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key != 'caption':\n",
    "            processed_batch[key] = torch.stack([item[key] for item in batch])\n",
    "        else:\n",
    "            tokenized_caption = processor.tokenizer(\n",
    "                [item[key] for item in batch],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            processed_batch['input_ids'] = tokenized_caption['input_ids']\n",
    "    \n",
    "    return processed_batch\n",
    "\n",
    "\n",
    "\n",
    "# load train set\n",
    "train_ds = load_from_disk('../dataset/train_dataset')\n",
    "\n",
    "# convert a huggingface dataset type to pytorch dataset type\n",
    "train_ds_pt = ImageCaptionDataset(train_ds, processor=processor)\n",
    "train_dataloader = DataLoader(train_ds_pt, shuffle=True, batch_size=training_args.per_device_train_batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(adapter_model.parameters(), lr=1e-5, eps=1e-5)\n",
    "num_training_steps = len(train_dataloader) * 2  # Assume 2 epochs\n",
    "num_warmup_steps = int(num_training_steps * 0.1)  # 10% warmup\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "adapter_model.to(device)\n",
    "adapter_model.half()\n",
    "adapter_model.train()\n",
    "\n",
    "epoch_loss = 0.0\n",
    "round_loss = 0.0\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device, torch.float16)\n",
    "        outputs = adapter_model(\n",
    "            input_ids = input_ids,\n",
    "            pixel_values = pixel_values, \n",
    "            labels = input_ids\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        round_loss += loss.item()\n",
    "        if (step+1) % 40 == 0: \n",
    "            # Extract logits from the model output\n",
    "            logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            # Get predicted token IDs by taking the argmax over the vocab dimension\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "            # Limit the number of tokens to 30 before decoding\n",
    "            seq_length = 10\n",
    "            predicted_ids = predicted_ids[:, :seq_length]  # Truncate to max 30 tokens\n",
    "            # Convert token IDs to text using tokenizer\n",
    "            decoded_output = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            # Print generated text\n",
    "            print(\"Generated Output:\", decoded_output)\n",
    "\n",
    "            # Print avg loss for 40 training samples\n",
    "            print(round_loss/40)\n",
    "            round_loss = 0.0\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch} - Avg Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "    epoch_loss = 0.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pbe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
