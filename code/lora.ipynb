{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wln/miniconda3/envs/llm-pbe/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# set os at top \n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['HF_HOME'] = '/data1/wln/hf_cache'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# load model and processor\n",
    "processor = AutoProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
    "base_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    'Salesforce/blip2-opt-2.7b', \n",
    "    local_files_only=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# set training args \n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "# lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16, #8\n",
    "    lora_alpha=32, #16 \n",
    "    lora_dropout=0.1, #0.05\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# get model for training\n",
    "adapter_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7033\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"parquet\", \n",
    "    data_files=['../dataset/0000.parquet', '../dataset/0001.parquet']\n",
    ")\n",
    "\n",
    "# split\n",
    "train_ds = ds.filter(lambda x:x['split'] == 'train', num_proc=32)['train']\n",
    "print(len(train_ds))\n",
    "val_ds = ds.filter(lambda x: x['split'] == 'val', num_proc=32)['train']\n",
    "test_ds = ds.filter(lambda x: x['split'] == 'test', num_proc=32)['train']\n",
    "\n",
    "# convert a huggingface dataset type to pytorch dataset type\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataset[index]\n",
    "        instruct = \"A short image caption:\"\n",
    "        encoding = self.processor(\n",
    "            images=item['image'], \n",
    "            text=instruct, \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding['label'] = item['caption'][0]\n",
    "        return encoding\n",
    "\n",
    "train_ds_pt = ImageCaptionDataset(train_ds, processor=processor)\n",
    "\n",
    "def collator(batch):\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key != 'label':\n",
    "            processed_batch[key] = torch.stack([b[key] for b in batch])\n",
    "        else:\n",
    "            labels = [b['label'] for b in batch]\n",
    "            labels_pt = processor.tokenizer(\n",
    "                labels,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "            labels_pt[\"input_ids\"][labels_pt[\"input_ids\"] == processor.tokenizer.pad_token_id] = -100\n",
    "            processed_batch['labels_ids'] = labels_pt['input_ids']\n",
    "            processed_batch['labels_attention_mask'] = labels_pt['attention_mask']\n",
    "    \n",
    "    return processed_batch\n",
    "train_dataloader = DataLoader(train_ds_pt, shuffle=True, batch_size=training_args.per_device_train_batch_size, collate_fn=collator)\n",
    "# batch = next(iter(train_dataloader))\n",
    "# # Print the keys in the batch\n",
    "# print(\"Batch Keys:\", batch.keys())\n",
    "\n",
    "# # Print shapes and types of each item in the batch\n",
    "# for key, value in batch.items():\n",
    "#     print(f\"\\nKey: {key}\")\n",
    "#     print(f\"Type: {type(value)}\")\n",
    "#     print(f\"Shape: {value.shape if isinstance(value, torch.Tensor) else 'N/A'}\")\n",
    "#     print(f\"Sample Data: {value[0] if isinstance(value, torch.Tensor) else value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/data1/wln/miniconda3/envs/llm-pbe/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/data1/wln/miniconda3/envs/llm-pbe/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 7.6420300102233885\n",
      "Step 200, Loss: 6.096641244888306\n",
      "Step 300, Loss: 5.741399111747742\n",
      "Step 400, Loss: 5.573953895568848\n",
      "Step 500, Loss: 5.532182145118713\n",
      "Step 600, Loss: 5.477182502746582\n",
      "Step 700, Loss: 5.425440158843994\n",
      "Step 800, Loss: 5.351293129920959\n",
      "Step 900, Loss: 5.300527973175049\n",
      "Step 1000, Loss: 5.256822524070739\n",
      "Step 1100, Loss: 5.234334244728088\n",
      "Step 1200, Loss: 5.34716121673584\n",
      "Step 1300, Loss: 5.271569843292236\n",
      "Step 1400, Loss: 5.262822613716126\n",
      "Step 1500, Loss: 5.254798636436463\n",
      "Step 1600, Loss: 5.2519832038879395\n"
     ]
    }
   ],
   "source": [
    "from torch import autograd\n",
    "from torch.amp import autocast, GradScaler\n",
    "from transformers import get_scheduler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(adapter_model.parameters(), lr=1e-5, eps=1e-5)\n",
    "num_training_steps = len(train_dataloader) * 2  # Assume 2 epochs\n",
    "num_warmup_steps = int(num_training_steps * 0.1)  # 10% warmup\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Use mixed precision\n",
    "scaler = GradScaler()\n",
    "# lora train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "adapter_model.to(device)\n",
    "\n",
    "## pad\n",
    "from torch.nn.functional import pad\n",
    "def pad_to_length(tensor, max_length, pad_value=None):\n",
    "    pad_value = pad_value if pad_value is not None else processor.tokenizer.pad_token_id\n",
    "    # Ensure tensor is at least 2D\n",
    "    if tensor.dim() == 1:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    # Pad or truncate tensor to max_length\n",
    "    return torch.nn.functional.pad(\n",
    "        tensor, (0, max_length - tensor.shape[1]), value=pad_value\n",
    "    ) if tensor.shape[1] < max_length else tensor[:, :max_length]\n",
    "## save memory \n",
    "if training_args.gradient_checkpointing:\n",
    "    adapter_model.gradient_checkpointing_enable()\n",
    "\n",
    "## train mode\n",
    "adapter_model.train()\n",
    "loss_list=[]\n",
    "for epoch in range(2):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    sum_loss_list = []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        ## Forward pass\n",
    "        input_ids = batch.pop('input_ids').squeeze(1) #instruct: \"A short image caption:\"\n",
    "        pixel_values = batch['pixel_values'].squeeze(1) # encoded pixel_values\n",
    "        labels = batch['labels_ids'].squeeze(1) # label tensors\n",
    "\n",
    "        # Padding input_ids with 0 (default padding token for most tokenizers)\n",
    "        input_ids = pad_to_length(input_ids, 128, pad_value=-100)\n",
    "        # Padding attention_mask with 0 (default padding token for most tokenizers)\n",
    "        attention_mask = pad_to_length(attention_mask, 128, pad_value=-100) # \"a/m for instruct\"\n",
    "        # # Padding labels with -100 to ignore them in the loss function\n",
    "        # labels = pad_to_length(labels, 128, pad_value=-100)\n",
    "\n",
    "        ## feed to model\n",
    "        with autocast():\n",
    "            outputs = adapter_model(\n",
    "                input_ids = input_ids,\n",
    "                pixel_values = pixel_values, \n",
    "                labels = labels, \n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "        sum_loss_list.append(float(loss.item()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Backward pass \n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        clip_grad_norm_(adapter_model.parameters(), max_norm=1.0)\n",
    "        ## update weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if step % 10 == 0: \n",
    "            generated_output = adapter_model.generate(pixel_values=pixel_values, max_new_tokens=20)\n",
    "            print(\"Generated caption:\", processor.batch_decode(generated_output, skip_special_tokens=True))\n",
    "\n",
    "    avg_sum_loss = sum(sum_loss_list) / len(sum_loss_list)\n",
    "    print(f\"Epoch {epoch} - Avg Loss: {avg_sum_loss}\")\n",
    "    loss_list.append(avg_sum_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pbe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
